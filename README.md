# E-Commerce Data Pipeline

## ğŸ“Œ Overview
This project implements an **end-to-end data pipeline** for an e-commerce system, transforming raw transactional data into analytics-ready data marts.

The main goal of this project is to demonstrate **data engineering fundamentals**:
- data ingestion
- layered data modeling (raw â†’ staging â†’ marts)
- reproducible pipelines
- basic data quality checks
- clear project structure

This is **not a dashboard or BI project**, but a **pipeline-focused data engineering project**.

---

## ğŸ§± Architecture Overview
```text
Raw Data (CSV)
â†“
Ingestion Layer
â†“
Staging Layer (cleaned & standardized)
â†“
Marts Layer (analytics-ready tables)
```

Each layer is executed as an independent step and orchestrated locally using a `Makefile`.

---

## ğŸ“‚ Project Structure
```text
ecommerce-data-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Raw source data (CSV / JSON)
â”‚ â”œâ”€â”€ staging/ # Cleaned and standardized data
â”‚ â””â”€â”€ marts/ # Analytics-ready data marts
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ ingestion/ # Raw data ingestion logic
â”‚ â”œâ”€â”€ staging/ # Raw â†’ staging transformations
â”‚ â”œâ”€â”€ marts/ # Build fact & dimension tables
â”‚ â””â”€â”€ utils/ # Shared utilities (logging, helpers)
â”‚
â”œâ”€â”€ sql/
â”‚ â”œâ”€â”€ ddl/ # Table definitions
â”‚ â””â”€â”€ marts/ # Analytics queries (optional)
â”‚
â”œâ”€â”€ tests/ # Basic data quality checks
â”œâ”€â”€ config/ # Pipeline configuration
â”‚
â”œâ”€â”€ Makefile # Pipeline orchestration
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

## ğŸ“Š Data Model (High Level)

This pipeline is built around three core entities:

- **customers**
- **orders**
- **order_items**

Relationships:
- One customer can have many orders
- One order can have many order items

The data is intentionally modeled in multiple layers to simulate real-world data pipelines.

---

## âš™ï¸ Pipeline Steps

### 1ï¸âƒ£ Ingestion
- Reads source data
- Splits and stores entity-based raw tables
- Writes output to `data/raw/`

### 2ï¸âƒ£ Staging
- Cleans and standardizes raw data
- Handles missing values and basic validation
- Writes output to `data/staging/`

### 3ï¸âƒ£ Marts
- Builds analytics-ready fact and dimension tables
- Writes output to `data/marts/`

---

## ğŸ§ª Data Quality Checks
Basic data quality validations are applied, such as:
- non-empty datasets
- non-null primary keys
- referential integrity between tables

These checks ensure that downstream data is reliable.

---

## â–¶ï¸ How to Run the Pipeline

All pipeline steps are orchestrated using a `Makefile`.

Run individual steps:
```bash
make ingest
make staging
make marts
```

Run the full pipeline:
```bash
make run-all
```

---

## ğŸ› ï¸ Tech Stack

- Python
- Pandas
- SQL
- Makefile (local orchestration)

---

## ğŸ¯ Project Scope

This project focuses on:

- pipeline structure
- data modeling
- reproducibility
- engineering best practices

---

## ğŸ“Œ Notes

This project is designed as a learning and portfolio project to demonstrate data engineering concepts in a clear and structured way.